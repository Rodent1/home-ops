---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: loki.rules
  namespace: monitoring
spec:
  groups:
    - name: loki.rules
      rules:
        - alert: LokiRequestErrors
          annotations:
            message: "{{ $labels.job }} {{ $labels.route }} is experiencing {{ $value | humanizePercentage }} errors."
          expr: |
            100 * sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[1m])) by (namespace, job, route)
              /
            sum(rate(loki_request_duration_seconds_count[1m])) by (namespace, job, route)
              > 10
          for: 15m
          labels:
            severity: critical
        - alert: LokiRequestPanics
          annotations:
            message: "{{ $labels.job }} is experiencing {{ $value | humanizePercentage }} increase of panics."
          expr: |
            sum(increase(loki_panic_total[10m])) by (namespace, job)
              > 0
          labels:
            severity: critical
        - alert: LokiRequestLatency
          annotations:
            message: "{{ $labels.job }} {{ $labels.route }} is experiencing {{ $value }}s 99th percentile latency."
          expr: |
            namespace_job_route:loki_request_duration_seconds:99quantile{route!~"(?i).*tail.*"}
              > 1
          for: 15m
          labels:
            severity: critical
        - expr: |
            histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket[1m]))
            by (le, job))
          record: job:loki_request_duration_seconds:99quantile
        - expr: |
            histogram_quantile(0.50, sum(rate(loki_request_duration_seconds_bucket[1m]))
            by (le, job))
          record: job:loki_request_duration_seconds:50quantile
        - expr: |
            sum(rate(loki_request_duration_seconds_sum[1m])) by (job)
              /
            sum(rate(loki_request_duration_seconds_count[1m])) by (job)
          record: job:loki_request_duration_seconds:avg
        - expr: |
            sum(rate(loki_request_duration_seconds_bucket[1m]))
            by (le, job)
          record: job:loki_request_duration_seconds_bucket:sum_rate
        - expr: |
            sum(rate(loki_request_duration_seconds_sum[1m])) by (job)
          record: job:loki_request_duration_seconds_sum:sum_rate
        - expr: |
            sum(rate(loki_request_duration_seconds_count[1m])) by (job)
          record: job:loki_request_duration_seconds_count:sum_rate
        - expr: |
            histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket[1m]))
            by (le, job, route))
          record: job_route:loki_request_duration_seconds:99quantile
        - expr: |
            histogram_quantile(0.50, sum(rate(loki_request_duration_seconds_bucket[1m]))
            by (le, job, route))
          record: job_route:loki_request_duration_seconds:50quantile
        - expr: |
            sum(rate(loki_request_duration_seconds_sum[1m])) by (job, route)
              /
            sum(rate(loki_request_duration_seconds_count[1m])) by (job, route)
          record: job_route:loki_request_duration_seconds:avg
        - expr: |
            sum(rate(loki_request_duration_seconds_bucket[1m]))
            by (le, job, route)
          record: job_route:loki_request_duration_seconds_bucket:sum_rate
        - expr: |
            sum(rate(loki_request_duration_seconds_sum[1m])) by (job, route)
          record: job_route:loki_request_duration_seconds_sum:sum_rate
        - expr: |
            sum(rate(loki_request_duration_seconds_count[1m])) by (job, route)
          record: job_route:loki_request_duration_seconds_count:sum_rate
        - expr: |
            histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket[1m]))
            by (le, namespace, job, route))
          record: namespace_job_route:loki_request_duration_seconds:99quantile
        - expr: |
            histogram_quantile(0.50, sum(rate(loki_request_duration_seconds_bucket[1m]))
            by (le, namespace, job, route))
          record: namespace_job_route:loki_request_duration_seconds:50quantile
        - expr: |
            sum(rate(loki_request_duration_seconds_sum[1m])) by (namespace, job, route)
              /
            sum(rate(loki_request_duration_seconds_count[1m])) by (namespace, job, route)
          record: namespace_job_route:loki_request_duration_seconds:avg
        - expr: |
            sum(rate(loki_request_duration_seconds_bucket[1m]))
            by (le, namespace, job, route)
          record: namespace_job_route:loki_request_duration_seconds_bucket:sum_rate
        - expr: |
            sum(rate(loki_request_duration_seconds_sum[1m]))
            by (namespace, job, route)
          record: namespace_job_route:loki_request_duration_seconds_sum:sum_rate
        - expr: |
            sum(rate(loki_request_duration_seconds_count[1m]))
            by (namespace, job, route)
          record: namespace_job_route:loki_request_duration_seconds_count:sum_rate
    - name: smart
      rules:
        - alert: SMARTFailure
          expr: |
            sum by (hostname) (count_over_time({hostname=~".+"} | json | _SYSTEMD_UNIT = "smartmontools.service" !~ "(?i)previous self-test completed without error" !~ "(?i)Prefailure" |~ "(?i)(error|fail)"[2m])) > 0
          for: 2m
          labels:
            severity: critical
            category: logs
          annotations:
            hostname: "{{ $labels.hostname }}"
            summary: "{{ $labels.hostname }} has reported SMART failures"

    - name: zigbee2mqtt
      rules:
        - alert: ZigbeeMQTTUnreachable
          expr: |
            sum(count_over_time({app="zigbee2mqtt"} |~ "(?i)not connected to mqtt server"[2m])) > 0
          for: 2m
          labels:
            severity: critical
            category: logs
          annotations:
            app: "{{ $labels.app }}"
            summary: "{{ $labels.app }} is unable to reach MQTT"

    - name: zwave-js-ui
      rules:
        - alert: ZwaveMQTTUnreachable
          expr: |
            sum(count_over_time({app="zwave-js-ui"} |~ "(?i)error while connecting mqtt"[2m])) > 0
          for: 2m
          labels:
            severity: critical
            category: logs
          annotations:
            app: "{{ $labels.app }}"
            summary: "{{ $labels.app }} is unable to reach MQTT"

    - name: frigate
      rules:
        - alert: FrigateMQTTUnreachable
          expr: |
            sum(count_over_time({app="frigate"} |~ "(?i)unable to connect to mqtt server"[2m])) > 0
          for: 2m
          labels:
            severity: critical
            category: logs
          annotations:
            app: "{{ $labels.app }}"
            summary: "{{ $labels.app }} is unable to reach MQTT"

    - name: autobrr
      rules:
        - alert: AutobrrNetworkUnhealthy
          expr: |
            sum by (app) (count_over_time({app="autobrr"} |~ "(?i)network unhealthy"[2m])) > 0
          for: 2m
          labels:
            severity: critical
            category: logs
          annotations:
            app: "{{ $labels.app }}"
            summary: "{{ $labels.app }} has a unhealthy network"

    - name: plex
      rules:
        - alert: PlexDatabaseBusy
          expr: |
            sum by (app) (count_over_time({app="plex"} |~ "(?i)retry busy DB"[2m])) > 0
          for: 2m
          labels:
            severity: critical
            category: logs
          annotations:
            app: "{{ $labels.app }}"
            summary: "{{ $labels.app }} is experiencing database issues"

    - name: home-assistant
      rules:
        - alert: HomeAssistantPostgresUnreachable
          expr: |
            sum by (app) (count_over_time({app="home-assistant"} |~ "(?i)error in database connectivity"[2m])) > 0
          for: 2m
          labels:
            severity: critical
            category: logs
          annotations:
            app: "{{ $labels.app }}"
            summary: "{{ $labels.app }} is unable to connect to postgres"
